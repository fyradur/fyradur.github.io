[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Pivoting in SQL\n\n\n\nSQL\n\n\n\n\n\n\n\n\n\nJun 11, 2025\n\n\nJohn Möller\n\n\n\n\n\n\n\n\n\n\n\n\nRake and Word Cloud\n\n\n\nSelf-Study Notes\n\nPython\n\nData Visualization\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nJohn Möller\n\n\n\n\n\n\n\n\n\n\n\n\nHTMLParser Basics\n\n\n\nSelf-Study Notes\n\nPython\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nJohn Möller\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RakeAndWordCloud/index.html",
    "href": "posts/RakeAndWordCloud/index.html",
    "title": "Rake and Word Cloud",
    "section": "",
    "text": "Introduction\nGoogling about a technical topic and browsing images one might have come across a word cloud; a way to visualize the frequencies of words in some text by size. Below is a word cloud of some text about linear algebra (I would guess it is the text from the Wikipedia article).\n\n\n\nLinear Algebra Word Cloud\n\n\nTo create such an image, we will use the package Rake, which transforms a text into a list of pairs consisting of keywords and their respective frequencies. Then we will visualize it using the package WordCloud.\n\n\nScraping Text from Wikipedia\nFirst off, let’s get a text which we can base our word cloud on. The following code scrapes all the readable text from the Wikipedia page on Data Science. It is explained in this previous blog post: HTMLParser Basics.\n\nimport requests\nfrom html.parser import HTMLParser\n\n\nurl = 'https://en.wikipedia.org/wiki/Data_science'\ntext = requests.get(url).content.decode('utf-8')\n\n\nclass MyHTMLParser(HTMLParser):\n    script = False\n    res = \"\"\n    def handle_starttag(self, tag, attrs):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = True\n    def handle_endtag(self, tag):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = False\n    def handle_data(self, data):\n        if str.strip(data)==\"\" or self.script:\n            return\n        # We also in this example choose to remove [ edit ]\n        # in the following way.\n        self.res += ' '+data.replace('[ edit ]','')\n\n\nparser = MyHTMLParser()\nparser.feed(text)\ntext = parser.res\n\n# Let's see the beginning of the text.\nprint(text[:1000])\n\n Data science - Wikipedia Jump to content Main menu Main menu move to sidebar hide \n        Navigation\n     Main page Contents Current events Random article About Wikipedia Contact us Donate \n        Contribute\n     Help Learn to edit Community portal Recent changes Upload file Languages Language links are at the top of the page. Search Search Create account Log in Personal tools Create account Log in \n        Pages for logged out editors  learn more Contributions Talk Contents move to sidebar hide (Top) 1 Foundations Toggle Foundations subsection 1.1 Relationship to statistics 2 Etymology Toggle Etymology subsection 2.1 Early usage 2.2 Modern usage 3 Data Science and Data Analysis 4 History 5 See also 6 References Toggle the table of contents Data science 46 languages العربية Azərbaycanca বাংলা Български Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Bahasa Indonesia IsiZulu Italiano עברית ಕನ್ನಡ Қазақша Latviešu Македонски Bahasa Melayu မြန်မာဘာသာ Ned\n\n\n\n\nGetting Key Word Frequencies Using Rake\nThe package nlp_rake provides a method Rake that creates an extractor object. We can pass our text into this object using its apply method, which will yield a list of keyword-frequency pairs.\nWhen creating the extractor object, we specify the maximum number of words a keyword can consist of, the minimum frequency for a keyword, and the minimum number of characters a keyword can have. In the following code, we use the values 2, 3, and 5, respectively.\n\nimport nlp_rake\nextractor = nlp_rake.Rake(max_words=2,min_freq=3,min_chars=5)\nres = extractor.apply(text)\nres\n\n[('data scientist', 4.0),\n ('data visualization', 4.0),\n ('machine learning', 4.0),\n ('data mining', 4.0),\n ('sexiest job', 4.0),\n ('21st century', 4.0),\n ('big data', 4.0),\n ('data scientists', 4.0),\n ('data science', 3.925373134328358),\n ('information science', 3.925373134328358),\n ('computer science', 3.925373134328358),\n ('statistical learning', 3.9),\n ('^ davenport', 3.8),\n ('data analysis', 3.75),\n ('science', 1.9253731343283582),\n ('analysis', 1.75),\n ('insights', 1.6666666666666667),\n ('field', 1.4285714285714286),\n ('computational', 1.25),\n ('statistics', 1.2173913043478262),\n ('thomas', 1.2),\n ('mathematics', 1.0),\n ('education', 1.0),\n ('communications', 1.0),\n ('archived', 1.0),\n ('original', 1.0),\n ('chikio', 1.0),\n ('forbes', 1.0)]\n\n\n\n\nCreating a Word Cloud\nThe wordcloud provides us the method WordCloud to create our word cloud object. This object has a method called generate_from_frequencies but it doesn’t accept the list format our res variable has. It needs to be in a dictionary format, so we’ll need to convert it to a dictionary before passing it in.\nAfterwards, one can save the image of this object to a filepath using the to_file method. But to display it in Jupyter, we will use matplotlib to create a figure and then use the method imshow.\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nwc = WordCloud(background_color='white',width=800,height=600)\nplt.figure(figsize=(15,7))\nplt.imshow(wc.generate_from_frequencies({ k:v for k,v in res }))\n\n\n\n\n\n\n\n\n\n\nMaking a Word Cloud from Raw Text\nYou can also pass raw text directly into the word cloud object using the generate method. However, this approach considers the English language in general, which may not be useful when investigating keywords related to a specific topic.\n\nplt.imshow(wc.generate(text))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Blog of John Möller",
    "section": "",
    "text": "I am a bachelore’s student of mathematics at Stockholm University. On this blog I document my progress in learning about math, programming and data science."
  },
  {
    "objectID": "posts/HTMLParserBasics/index.html",
    "href": "posts/HTMLParserBasics/index.html",
    "title": "HTMLParser Basics",
    "section": "",
    "text": "Let’s assume we have some html text. For example we might’ve gotten it the following way.\n\nimport requests\n\nurl = 'https://en.wikipedia.org/wiki/Data_science'\ntext = requests.get(url).content.decode('utf-8')\n\n# Let's see the beggining of this text\nprint(text[:1000])\n\n&lt;!DOCTYPE html&gt;\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;title&gt;Data science - Wikipedia&lt;/title&gt;\n&lt;script&gt;(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-c\n\n\nNow let’s say our goal is to extract all the text inside all tags except for those inside &lt;script&gt; and &lt;style&gt; tags.\nOne way to achieve this goal is to make a parser object. Then the idea is that if we have our parser parser then we can run parser.feed(text) and we can then get our result text from parser.res.\nTo create this object we will create a class that inherits from the class HTMLParser. It is from this class we will inherit the feed method. My understanding of what happens when it is called is that it goes through the text in order but in a chunked up way such that tags and data inside tags are seen as atomic elements. And it runs a corresponding function based on if that atomic element is a start tag, end tag or data. Also for each atomic tag element there seems to be the method lower available, which extracts the name of the tag in the form of a string.\nThus we can create our custom parser and get the parsed text as following.\n\nfrom html.parser import HTMLParser\n\nclass MyHTMLParser(HTMLParser):\n    script = False\n    res = \"\"\n    def handle_starttag(self, tag, attrs):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = True\n    def handle_endtag(self, tag):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = False\n    def handle_data(self, data):\n        if str.strip(data)==\"\" or self.script:\n            return\n        # We also in this example choose to remove [ edit ]\n        # in the following way.\n        self.res += ' '+data.replace('[ edit ]','')\n\nparser = MyHTMLParser()\nparser.feed(text)\ntext = parser.res\nprint(text[:1000])\n\n Data science - Wikipedia Jump to content Main menu Main menu move to sidebar hide \n        Navigation\n     Main page Contents Current events Random article About Wikipedia Contact us Donate \n        Contribute\n     Help Learn to edit Community portal Recent changes Upload file Languages Language links are at the top of the page. Search Search Create account Log in Personal tools Create account Log in \n        Pages for logged out editors  learn more Contributions Talk Contents move to sidebar hide (Top) 1 Foundations Toggle Foundations subsection 1.1 Relationship to statistics 2 Etymology Toggle Etymology subsection 2.1 Early usage 2.2 Modern usage 3 Data Science and Data Analysis 4 History 5 See also 6 References Toggle the table of contents Data science 46 languages العربية Azərbaycanca বাংলা Български Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Bahasa Indonesia IsiZulu Italiano עברית ಕನ್ನಡ Қазақша Latviešu Македонски Bahasa Melayu မြန်မာဘာသာ Ned"
  },
  {
    "objectID": "posts/SalesBlocket/template.html",
    "href": "posts/SalesBlocket/template.html",
    "title": "Modelling time until purchase on Blocket",
    "section": "",
    "text": "Background\nThe online second hand market has a lot of online websites. In Sweden a popular one is Blocket. If you search a certain item you can see all other sellers and their prices and when they published the advert. But what you can’t see is previous sold products. What if you want to have an idea of if you publish a certain product with a certain price how long will it take until you sell it? In this post we’ll try to estimate this value using available data.\n\n\nNotation\nAll uppercase variables will be assumed to be random variables and the lower case of it will be the expected value.\n\n\nVariables\n\nTime until sold (a function of set price)\nTime until seller takes down a post (a function of set price)"
  },
  {
    "objectID": "posts/PivotingSQL/template.html",
    "href": "posts/PivotingSQL/template.html",
    "title": "Pivoting in SQL",
    "section": "",
    "text": "Long and wide data\nConsider these two tables (made up data):\n\n\n\n‘Longer’ table\n\n\nStore\nDay\nStatus\n\n\n\n\nMedia Markt\nWeekday\nOpen\n\n\nMedia Markt\nWeekend\nClosed\n\n\nElgiganten\nWeekday\nOpen\n\n\nElgiganten\nWeekend\nOpen\n\n\nExpert\nWeekday\nClosed\n\n\nExpert\nWeekend\nClosed\n\n\n\n\n\n\n\n\n‘Wider’ table\n\n\nStore\nWeekday\nWeekend\n\n\n\n\nMedia Markt\nOpen\nClosed\n\n\nElgiganten\nOpen\nOpen\n\n\nExpert\nClosed\nClosed\n\n\n\n\n\nThey contain the same kind of information (except perhaps specifying the open status as specifically “Status”). Each of these forms have different advantages, thus it would be useful to be able to convert between these two formats in SQL.\n\n\nPivot longer\nConsider the table birthstatistics which first lines looks like this:\n\nSELECT * FROM birthstatistics\nLIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nsex\nforeign/Swedish background\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n\n\n\n\n01 Stockholm county\nboys\nforeign born\n13346\n13157\n12759\n12675\n13271\n14325\n15126\n16074\n16901\n18075\n19050\n20059\n20827\n21843\n23084\n25124\n26270\n27060\n26484\n\n\n01 Stockholm county\nboys\nborn in Sweden with two foreign born parents\n31596\n32185\n32785\n33456\n34282\n35224\n36180\n37107\n38221\n39262\n40386\n41639\n43249\n45001\n46696\n48375\n50232\n52305\n53741\n\n\n01 Stockholm county\nboys\nborn in Sweden with one parent born in Sweden and one foreign born parent\n29815\n30395\n30976\n31565\n32102\n32667\n33368\n34193\n35105\n35894\n36845\n37945\n39228\n40391\n41558\n42512\n43310\n44123\n44619\n\n\n01 Stockholm county\nboys\nborn in Sweden with two parents born in Sweden\n131708\n133236\n134604\n135502\n136532\n137152\n137235\n137895\n138408\n138634\n138838\n140035\n141358\n142826\n143973\n144656\n144897\n144727\n144212\n\n\n01 Stockholm county\ngirls\nforeign born\n12946\n12732\n12570\n12570\n13376\n14356\n15184\n16107\n16738\n17719\n18685\n19278\n19976\n20694\n21615\n23256\n24556\n25475\n24805\n\n\n\n\n\nLet’s say we want to do a bunch of operations where we specify mathematical constraints on the year number. Let’s say one of them is to get the data where the year is an even number. If there was a column called Year we could specify “WHERE Year % 2 = 0”. But we can’t do that as each year is a column and not a cell value in a column. Thus we would like instead a form where there was a column called “Year”. Then we can have the values in each cell be inside a single column that we will call “Persons”. This is one way to achieve that in SQL (not the prettiest way, but one of the most straightforward):\n\nCREATE View long_table AS\nSELECT region, sex, 'foreign/Swedish', 2002 as Year, [2002] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2003 as Year, [2003] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2004 as Year, [2004] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2005 as Year, [2005] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2006 as Year, [2006] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2007 as Year, [2007] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2008 as Year, [2008] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2009 as Year, [2009] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2010 as Year, [2010] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2011 as Year, [2011] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2012 as Year, [2012] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2013 as Year, [2013] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2014 as Year, [2014] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2015 as Year, [2015] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2016 as Year, [2016] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2017 as Year, [2017] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2018 as Year, [2018] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2019 as Year, [2019] as Persons FROM birthstatistics\nUNION ALL\nSELECT region, sex, 'foreign/Swedish', 2020 as Year, [2020] as Persons FROM birthstatistics;\n\nNow we can do a statement like the following:\n\nSELECT * FROM long_table\nWHERE Year % 2 = 0 AND sex = \"girls\" AND Persons &gt; 100000\nLIMIT 5;\n\n\n5 records\n\n\nregion\nsex\n‘foreign/Swedish’\nYear\nPersons\n\n\n\n\n01 Stockholm county\ngirls\nforeign/Swedish\n2002\n125531\n\n\n14 Västra Götaland county\ngirls\nforeign/Swedish\n2002\n120074\n\n\n01 Stockholm county\ngirls\nforeign/Swedish\n2004\n127747\n\n\n14 Västra Götaland county\ngirls\nforeign/Swedish\n2004\n118544\n\n\n01 Stockholm county\ngirls\nforeign/Swedish\n2006\n129493\n\n\n\n\n\n\n\nPivot wider\nNow let’s say we want boys and girls to be columns. We can achieve that with the following:\n\nCREATE VIEW My_view AS\nSELECT region, 'foreign/Swedish', Year,\n  SUM(CASE WHEN sex = 'boys' THEN Persons ELSE 0 END) AS Boys,\n  SUM(CASE WHEN sex = 'girls' THEN Persons ELSE 0 END) AS Girls\nFROM long_table\nGROUP BY region, 'foreign/Swedish', Year\n\nWhy are we using SUM? Well in this example we kind of assume that given a distinct combination of the columns ‘region’, ‘foreign/Swedish’ and ‘Year’, then there only exists on row where ‘sex’ is specified as for example boy, and the value in ‘Persons’ is the value we’ll put under the column ‘boy’ in the widened table with that combination. But what if there are multiple? One way for example to deal with it is to just pick the first row and ignore the others. But one way that usually generalizes well is to sum all of them (perhaps each row symbolizes a sub-region for example).\nHere’s an example of a query using the the view with widened data:\n\nSELECT * FROM My_View\nWHERE Year % 2 = 0\nLIMIT 5\n\n\n5 records\n\n\nregion\n‘foreign/Swedish’\nYear\nBoys\nGirls\n\n\n\n\n01 Stockholm county\nforeign/Swedish\n2002\n206465\n196752\n\n\n01 Stockholm county\nforeign/Swedish\n2004\n211124\n200638\n\n\n01 Stockholm county\nforeign/Swedish\n2006\n216187\n205821\n\n\n01 Stockholm county\nforeign/Swedish\n2008\n221909\n210582\n\n\n01 Stockholm county\nforeign/Swedish\n2010\n228635\n216624\n\n\n\n\n\nAs an example let’s create two graphs for the total boys through the years and the total girls through the years.\n\nCREATE VIEW My_total AS\nSELECT Year, SUM(Boys) AS Boys, SUM(Girls) AS Girls\nFROM My_view\nGROUP BY Year\n\n\nlibrary(ggplot2)\nmy_view_df &lt;- dbGetQuery(con, \"SELECT * FROM My_total\")\nggplot(my_view_df, aes(Year, Boys)) + geom_line()\n\n\n\n\n\n\n\nggplot(my_view_df, aes(Year, Girls)) + geom_line()"
  },
  {
    "objectID": "posts/Nextreport/template.html",
    "href": "posts/Nextreport/template.html",
    "title": "What if We Knew Next Year’s Annual Reports Today?",
    "section": "",
    "text": "Introduction\nLet’s say you are an underpaid clerk for a big company, and during a routine day you are carrying a bunch of annual reports, when suddenly, you become magically teleported back 1 year in the past right in front of past you. Out of shock you drop all your reports, but before you even get to talk with your past self you are teleported back to the present. Now the past you have felt unmotivated previously to analyze data to predict stock prices because of the efficient-market hypothesis but now the past you recognizes this is potentially novel data that isn’t subject to that effect. So the question is now: Are these reports useful data for predicting stock price, and in that case, how do we use this data?\nIn this article we will investigate this question by looking at available previous reports and look how their metrics relate to the stock performance of the year it is describing. Then we will try to develop KPIs to these reports that will be useful for aiding us in how to buy stocks a year before the reports release."
  }
]