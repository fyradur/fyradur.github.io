[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fyradur",
    "section": "",
    "text": "Template\n\n\nPost description\n\n\n\n\n\nJun 10, 2025\n\n\nJohn Möller\n\n\n\n\n\n\n\n\n\n\n\n\nLoad Online CSV to Dataframe\n\n\n\nSelf-Study Notes\n\nPandas\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nJohn Möller\n\n\n\n\n\n\n\n\n\n\n\n\nRake and Word Cloud\n\n\n\nSelf-Study Notes\n\nPython\n\nData Visualization\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nJohn Möller\n\n\n\n\n\n\n\n\n\n\n\n\nHTMLParser Basics\n\n\n\nSelf-Study Notes\n\nPython\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nJohn Möller\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Blog of John Möller",
    "section": "",
    "text": "I am a bachelore’s student of mathematics at Stockholm University. On this blog I document my progress in learning about math, programming and data science."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/template/template.html",
    "href": "posts/template/template.html",
    "title": "Template",
    "section": "",
    "text": "This is how it looks like"
  },
  {
    "objectID": "posts/HTMLParserBasics/index.html",
    "href": "posts/HTMLParserBasics/index.html",
    "title": "HTMLParser Basics",
    "section": "",
    "text": "Let’s assume we have some html text. For example we might’ve gotten it the following way.\n\nimport requests\n\nurl = 'https://en.wikipedia.org/wiki/Data_science'\ntext = requests.get(url).content.decode('utf-8')\n\n# Let's see the beggining of this text\nprint(text[:1000])\n\n&lt;!DOCTYPE html&gt;\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;title&gt;Data science - Wikipedia&lt;/title&gt;\n&lt;script&gt;(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-c\n\n\nNow let’s say our goal is to extract all the text inside all tags except for those inside &lt;script&gt; and &lt;style&gt; tags.\nOne way to achieve this goal is to make a parser object. Then the idea is that if we have our parser parser then we can run parser.feed(text) and we can then get our result text from parser.res.\nTo create this object we will create a class that inherits from the class HTMLParser. It is from this class we will inherit the feed method. My understanding of what happens when it is called is that it goes through the text in order but in a chunked up way such that tags and data inside tags are seen as atomic elements. And it runs a corresponding function based on if that atomic element is a start tag, end tag or data. Also for each atomic tag element there seems to be the method lower available, which extracts the name of the tag in the form of a string.\nThus we can create our custom parser and get the parsed text as following.\n\nfrom html.parser import HTMLParser\n\nclass MyHTMLParser(HTMLParser):\n    script = False\n    res = \"\"\n    def handle_starttag(self, tag, attrs):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = True\n    def handle_endtag(self, tag):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = False\n    def handle_data(self, data):\n        if str.strip(data)==\"\" or self.script:\n            return\n        # We also in this example choose to remove [ edit ]\n        # in the following way.\n        self.res += ' '+data.replace('[ edit ]','')\n\nparser = MyHTMLParser()\nparser.feed(text)\ntext = parser.res\nprint(text[:1000])\n\n Data science - Wikipedia Jump to content Main menu Main menu move to sidebar hide \n        Navigation\n     Main page Contents Current events Random article About Wikipedia Contact us Donate \n        Contribute\n     Help Learn to edit Community portal Recent changes Upload file Languages Language links are at the top of the page. Search Search Create account Log in Personal tools Create account Log in \n        Pages for logged out editors  learn more Contributions Talk Contents move to sidebar hide (Top) 1 Foundations Toggle Foundations subsection 1.1 Relationship to statistics 2 Etymology Toggle Etymology subsection 2.1 Early usage 2.2 Modern usage 3 Data Science and Data Analysis 4 History 5 See also 6 References Toggle the table of contents Data science 46 languages العربية Azərbaycanca বাংলা Български Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Bahasa Indonesia IsiZulu Italiano עברית ಕನ್ನಡ Қазақша Latviešu Македонски Bahasa Melayu မြန်မာဘာသာ Ned"
  },
  {
    "objectID": "posts/LoadOnlineCSVToDataframe/index.html",
    "href": "posts/LoadOnlineCSVToDataframe/index.html",
    "title": "Load Online CSV to Dataframe",
    "section": "",
    "text": "The read_csv function from pandas can take both local file paths and URLs.\n\nimport pandas as pd\n\n# URL of the online CSV file\nurl = \"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\"\n\n# Load the CSV file into a dataframe\ndf = pd.read_csv(url)\n\n# Display the head of the dataframe\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "posts/RakeAndWordCloud/index.html",
    "href": "posts/RakeAndWordCloud/index.html",
    "title": "Rake and Word Cloud",
    "section": "",
    "text": "Introduction\nGoogling about a technical topic and browsing images one might have come across a word cloud; a way to visualize the frequencies of words in some text by size. Below is a word cloud of some text about linear algebra (I would guess it is the text from the Wikipedia article).\n\n\n\nLinear Algebra Word Cloud\n\n\nTo create such an image, we will use the package Rake, which transforms a text into a list of pairs consisting of keywords and their respective frequencies. Then we will visualize it using the package WordCloud.\n\n\nScraping Text from Wikipedia\nFirst off, let’s get a text which we can base our word cloud on. The following code scrapes all the readable text from the Wikipedia page on Data Science. It is explained in this previous blog post: HTMLParser Basics.\n\nimport requests\nfrom html.parser import HTMLParser\n\n\nurl = 'https://en.wikipedia.org/wiki/Data_science'\ntext = requests.get(url).content.decode('utf-8')\n\n\nclass MyHTMLParser(HTMLParser):\n    script = False\n    res = \"\"\n    def handle_starttag(self, tag, attrs):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = True\n    def handle_endtag(self, tag):\n        if tag.lower() in [\"script\",\"style\"]:\n            self.script = False\n    def handle_data(self, data):\n        if str.strip(data)==\"\" or self.script:\n            return\n        # We also in this example choose to remove [ edit ]\n        # in the following way.\n        self.res += ' '+data.replace('[ edit ]','')\n\n\nparser = MyHTMLParser()\nparser.feed(text)\ntext = parser.res\n\n# Let's see the beginning of the text.\nprint(text[:1000])\n\n Data science - Wikipedia Jump to content Main menu Main menu move to sidebar hide \n        Navigation\n     Main page Contents Current events Random article About Wikipedia Contact us Donate \n        Contribute\n     Help Learn to edit Community portal Recent changes Upload file Languages Language links are at the top of the page. Search Search Create account Log in Personal tools Create account Log in \n        Pages for logged out editors  learn more Contributions Talk Contents move to sidebar hide (Top) 1 Foundations Toggle Foundations subsection 1.1 Relationship to statistics 2 Etymology Toggle Etymology subsection 2.1 Early usage 2.2 Modern usage 3 Data Science and Data Analysis 4 History 5 See also 6 References Toggle the table of contents Data science 46 languages العربية Azərbaycanca বাংলা Български Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Bahasa Indonesia IsiZulu Italiano עברית ಕನ್ನಡ Қазақша Latviešu Македонски Bahasa Melayu မြန်မာဘာသာ Ned\n\n\n\n\nGetting Key Word Frequencies Using Rake\nThe package nlp_rake provides a method Rake that creates an extractor object. We can pass our text into this object using its apply method, which will yield a list of keyword-frequency pairs.\nWhen creating the extractor object, we specify the maximum number of words a keyword can consist of, the minimum frequency for a keyword, and the minimum number of characters a keyword can have. In the following code, we use the values 2, 3, and 5, respectively.\n\nimport nlp_rake\nextractor = nlp_rake.Rake(max_words=2,min_freq=3,min_chars=5)\nres = extractor.apply(text)\nres\n\n[('data scientist', 4.0),\n ('data visualization', 4.0),\n ('machine learning', 4.0),\n ('data mining', 4.0),\n ('sexiest job', 4.0),\n ('21st century', 4.0),\n ('big data', 4.0),\n ('data scientists', 4.0),\n ('data science', 3.925373134328358),\n ('information science', 3.925373134328358),\n ('computer science', 3.925373134328358),\n ('statistical learning', 3.9),\n ('^ davenport', 3.8),\n ('data analysis', 3.75),\n ('science', 1.9253731343283582),\n ('analysis', 1.75),\n ('insights', 1.6666666666666667),\n ('field', 1.4285714285714286),\n ('computational', 1.25),\n ('statistics', 1.2173913043478262),\n ('thomas', 1.2),\n ('mathematics', 1.0),\n ('education', 1.0),\n ('communications', 1.0),\n ('archived', 1.0),\n ('original', 1.0),\n ('chikio', 1.0),\n ('forbes', 1.0)]\n\n\n\n\nCreating a Word Cloud\nThe wordcloud provides us the method WordCloud to create our word cloud object. This object has a method called generate_from_frequencies but it doesn’t accept the list format our res variable has. It needs to be in a dictionary format, so we’ll need to convert it to a dictionary before passing it in.\nAfterwards, one can save the image of this object to a filepath using the to_file method. But to display it in Jupyter, we will use matplotlib to create a figure and then use the method imshow.\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nwc = WordCloud(background_color='white',width=800,height=600)\nplt.figure(figsize=(15,7))\nplt.imshow(wc.generate_from_frequencies({ k:v for k,v in res }))\n\n\n\n\n\n\n\n\n\n\nMaking a Word Cloud from Raw Text\nYou can also pass raw text directly into the word cloud object using the generate method. However, this approach considers the English language in general, which may not be useful when investigating keywords related to a specific topic.\n\nplt.imshow(wc.generate(text))"
  }
]